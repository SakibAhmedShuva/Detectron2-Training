{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4RYEY0fs1Po5",
        "outputId": "9bc01c86-c579-42ad-e2ef-26341d22d90c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0+cu126'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # install dependencies\n",
        "# !pip install -U torch torchvision cython\n",
        "# !pip install -U 'git+https://github.com/facebookresearch/fvcore.git' 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "# import torch, torchvision\n",
        "# torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "Dzu_yUwKvgsb",
        "outputId": "3cad645c-192c-4c83-9b80-5663cf2d60f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed antlr4-python3-runtime-4.9.3 black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0e40eab97e6544d1b9e431289cd4ecdf",
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/detectron2 detectron2\n",
        "!pip install -e detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6jmYDB_OoXd",
        "outputId": "f60b32dd-0f27-4d5b-b636-55a1aa2a7ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsBWE-TUR9ZX"
      },
      "outputs": [],
      "source": [
        "# !python -m pip install pyyaml==5.1\n",
        "# import sys, os, distutils.core\n",
        "# # Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# # See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "# !git clone 'https://github.com/facebookresearch/detectron2'\n",
        "# dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "# !python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "# sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# # Properly install detectron2. (Please do not install twice in both ways)\n",
        "# # !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3P12PEpdpd-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjddsG8rDP5B",
        "outputId": "0dd58632-0816-46b6-b8d8-700b6bd7d5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vWEbvfMEOx-06zQU_aSn12KQ2zXAD1W4\n",
            "From (redirected): https://drive.google.com/uc?id=1vWEbvfMEOx-06zQU_aSn12KQ2zXAD1W4&confirm=t&uuid=45144aac-a10c-411b-bfee-f4053ec0c063\n",
            "To: /content/SwimmingXDrowning.v3-v2.coco.zip\n",
            "100% 77.5M/77.5M [00:00<00:00, 162MB/s]\n"
          ]
        }
      ],
      "source": [
        "# data\n",
        "!gdown 1vWEbvfMEOx-06zQU_aSn12KQ2zXAD1W4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cReE2p1a6hDu"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# For files you've uploaded to Colab\n",
        "with zipfile.ZipFile('SwimmingXDrowning.v3-v2.coco.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/SwimmingXDrowning-3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoAc8yu5O2P1",
        "outputId": "8923db88-a12a-4804-ca04-c855d2cf3102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmErOGiZO-Zj",
        "outputId": "b4e430ad-30d7-4b2b-f2b0-b193d97e358a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory already exists: /content/drive/MyDrive/DS-OLD/RCNN NEW\n",
            "/content/drive/MyDrive/DS-OLD/RCNN NEW\n"
          ]
        }
      ],
      "source": [
        "# Go to YOLOv8 root folder\n",
        "import os\n",
        "\n",
        "target_dir = \"/content/drive/MyDrive/DS-OLD/RCNN NEW\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "    print(f\"Created directory: {target_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {target_dir}\")\n",
        "\n",
        "# Change to the directory\n",
        "%cd \"{target_dir}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-TTxsuOt4yl",
        "outputId": "757bc14f-ab2a-4eb8-8afd-fbce624e4975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1phc_2Po8qI-EXNFoIqFOSR9aYjqjIqXm inference\n",
            "Processing file 1ax7MylVDUNRSFRigwWCrrBLGrHst2vux coco_instances_results.json\n",
            "Processing file 1lnC09YdljlRR8NwAy2mmdR6vLTbzvw-0 instances_predictions.pth\n",
            "Processing file 1iqltE7ED10KQRtWEEQVBPk-m6KfZcEdL events.out.tfevents.1745442050.77945e44168f.3144.0\n",
            "Processing file 1vUWUsfnKErVUIJIGYxYS7qeKI9Th0udm events.out.tfevents.1745464094.27ec434b0b6d.3346.0\n",
            "Processing file 1fhthJnKfQYRQhEt5aFkazfxMnFD1YNWd last_checkpoint\n",
            "Processing file 1M5mzIsol_rQEiNDCeAn_h9zK3CMrXacu metrics.json\n",
            "Processing file 1B6CcdpT3hj2t4e3Di3wRExcJZUjkHi8H model_0004399.pth\n",
            "Processing file 1CVGHCqdI1l-vEBEpiLYXv9BjJT0s0QwR model_0004599.pth\n",
            "Processing file 1BgozITnR5X9lcWgAAWFXcmiUlpsPJQib model_0004799.pth\n",
            "Processing file 1gefifF8MJ8caNSVHMadiI1Exyt_lpUyW model_0004999.pth\n",
            "Processing file 1EetyVkfUPWExbrQ7BvgibZQggyd08RE2 model_0005199.pth\n",
            "Processing file 1Oub_-hJeWjutJF6ANuXADy_b1jTT-w7x model_0005399.pth\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ax7MylVDUNRSFRigwWCrrBLGrHst2vux\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/inference/coco_instances_results.json\n",
            "100% 145k/145k [00:00<00:00, 62.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lnC09YdljlRR8NwAy2mmdR6vLTbzvw-0\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/inference/instances_predictions.pth\n",
            "100% 77.6k/77.6k [00:00<00:00, 35.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iqltE7ED10KQRtWEEQVBPk-m6KfZcEdL\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/events.out.tfevents.1745442050.77945e44168f.3144.0\n",
            "100% 65.3k/65.3k [00:00<00:00, 89.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vUWUsfnKErVUIJIGYxYS7qeKI9Th0udm\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/events.out.tfevents.1745464094.27ec434b0b6d.3346.0\n",
            "100% 190k/190k [00:00<00:00, 73.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fhthJnKfQYRQhEt5aFkazfxMnFD1YNWd\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/last_checkpoint\n",
            "100% 17.0/17.0 [00:00<00:00, 71.7kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1M5mzIsol_rQEiNDCeAn_h9zK3CMrXacu\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/metrics.json\n",
            "100% 186k/186k [00:00<00:00, 76.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1B6CcdpT3hj2t4e3Di3wRExcJZUjkHi8H\n",
            "From (redirected): https://drive.google.com/uc?id=1B6CcdpT3hj2t4e3Di3wRExcJZUjkHi8H&confirm=t&uuid=dac585fd-ad44-45b4-bfd4-0e1353386dcb\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/model_0004399.pth\n",
            "100% 482M/482M [00:03<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CVGHCqdI1l-vEBEpiLYXv9BjJT0s0QwR\n",
            "From (redirected): https://drive.google.com/uc?id=1CVGHCqdI1l-vEBEpiLYXv9BjJT0s0QwR&confirm=t&uuid=3d458c71-a959-492a-a022-f2087e324368\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/model_0004599.pth\n",
            "100% 482M/482M [00:06<00:00, 75.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1BgozITnR5X9lcWgAAWFXcmiUlpsPJQib\n",
            "From (redirected): https://drive.google.com/uc?id=1BgozITnR5X9lcWgAAWFXcmiUlpsPJQib&confirm=t&uuid=361db853-8c84-45fb-8968-be1c988f9c15\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/model_0004799.pth\n",
            "100% 482M/482M [00:02<00:00, 164MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gefifF8MJ8caNSVHMadiI1Exyt_lpUyW\n",
            "From (redirected): https://drive.google.com/uc?id=1gefifF8MJ8caNSVHMadiI1Exyt_lpUyW&confirm=t&uuid=a60dc14b-0653-498f-85bc-209757ba4798\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/model_0004999.pth\n",
            "100% 482M/482M [00:04<00:00, 118MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EetyVkfUPWExbrQ7BvgibZQggyd08RE2\n",
            "From (redirected): https://drive.google.com/uc?id=1EetyVkfUPWExbrQ7BvgibZQggyd08RE2&confirm=t&uuid=6b36d5aa-1962-4e6e-a9df-41647c51425d\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/model_0005199.pth\n",
            "100% 482M/482M [00:05<00:00, 91.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Oub_-hJeWjutJF6ANuXADy_b1jTT-w7x\n",
            "From (redirected): https://drive.google.com/uc?id=1Oub_-hJeWjutJF6ANuXADy_b1jTT-w7x&confirm=t&uuid=36220c02-8d64-44b3-9ef1-cc60c90df8c1\n",
            "To: /content/drive/MyDrive/DS-OLD/RCNN NEW/output_detection/model_0005399.pth\n",
            "100% 482M/482M [00:06<00:00, 73.5MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "#Models\n",
        "!gdown --folder 1iLtBJsNhRtGHYB9LzzyeBYUSHYVbD9tg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy4vZo7KOPBs"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOBzwOV8ONrl",
        "outputId": "c15696d1-3f2b-456d-b3a0-1fef5b6689cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registering datasets...\n",
            "Registered 'swim_drown_train'\n",
            "Registered 'swim_drown_val'\n",
            "Dataset registration finished.\n",
            "\n",
            "--- Configuring Model ---\n",
            "Loaded base configuration from: COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\n",
            "Set model weights URL: https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl\n",
            "Set number of foreground classes: 2\n",
            "Set Images Per Batch (Batch Size): 14\n",
            "Checkpoints will be saved every 200 iterations.\n",
            "Output directory set to: ./output_detection\n",
            "\n",
            "--- Initializing Trainer ---\n",
            "[04/24 07:41:54 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (6): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (7): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (8): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (9): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (10): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (11): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (12): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (13): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (14): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (15): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (16): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (17): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (18): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (19): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (20): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (21): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (22): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[04/24 07:41:54 d2.data.datasets.coco]: Loaded 1258 images in COCO format from /content/SwimmingXDrowning-3/train/_annotations.coco.json\n",
            "[04/24 07:41:55 d2.data.build]: Removed 0 images with no usable annotations. 1258 images left.\n",
            "[04/24 07:41:55 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "|  Drowning  | 667          |  Swimming  | 1630         |\n",
            "|            |              |            |              |\n",
            "|   total    | 2297         |            |              |\n",
            "[04/24 07:41:55 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[04/24 07:41:55 d2.data.build]: Using training sampler TrainingSampler\n",
            "[04/24 07:41:55 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 07:41:55 d2.data.common]: Serializing 1258 elements to byte tensors and concatenating them all ...\n",
            "[04/24 07:41:55 d2.data.common]: Serialized dataset takes 0.38 MiB\n",
            "[04/24 07:41:55 d2.data.build]: Making batched data loader with batch_size=14\n",
            "[04/24 07:41:55 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output_detection/model_0005399.pth ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 07:41:56 d2.engine.hooks]: Loading scheduler from state_dict ...\n",
            "Trainer initialized and pre-trained weights requested for loading.\n",
            "\n",
            "--- Starting Training for 30000 iterations ---\n",
            "Checkpoints will be saved in: ./output_detection\n",
            "Evaluation will run every 200 iterations.\n",
            "[04/24 07:41:56 d2.engine.train_loop]: Starting training from iteration 5400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 07:43:06 d2.utils.events]:  eta: 22:40:13  iter: 5419  total_loss: 0.3793  loss_cls: 0.07507  loss_box_reg: 0.2907  loss_rpn_cls: 0.0006656  loss_rpn_loc: 0.00611    time: 3.3325  last_time: 3.2985  data_time: 0.2238  last_data_time: 0.2340   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:44:27 d2.utils.events]:  eta: 23:44:14  iter: 5439  total_loss: 0.3433  loss_cls: 0.06851  loss_box_reg: 0.268  loss_rpn_cls: 0.0005141  loss_rpn_loc: 0.005046    time: 3.4675  last_time: 3.7000  data_time: 0.2016  last_data_time: 0.3670   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:45:38 d2.utils.events]:  eta: 1 day, 0:09:24  iter: 5459  total_loss: 0.3332  loss_cls: 0.07023  loss_box_reg: 0.2673  loss_rpn_cls: 0.0002864  loss_rpn_loc: 0.005344    time: 3.4998  last_time: 3.5557  data_time: 0.1966  last_data_time: 0.1610   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:46:49 d2.utils.events]:  eta: 1 day, 0:13:02  iter: 5479  total_loss: 0.3714  loss_cls: 0.07537  loss_box_reg: 0.2865  loss_rpn_cls: 0.001209  loss_rpn_loc: 0.006054    time: 3.5130  last_time: 3.7098  data_time: 0.2050  last_data_time: 0.2764   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:48:01 d2.utils.events]:  eta: 1 day, 0:15:30  iter: 5499  total_loss: 0.3602  loss_cls: 0.06892  loss_box_reg: 0.2824  loss_rpn_cls: 0.0003789  loss_rpn_loc: 0.005338    time: 3.5238  last_time: 3.2312  data_time: 0.2009  last_data_time: 0.2086   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:49:13 d2.utils.events]:  eta: 1 day, 0:16:26  iter: 5519  total_loss: 0.389  loss_cls: 0.0781  loss_box_reg: 0.2951  loss_rpn_cls: 0.0008107  loss_rpn_loc: 0.006313    time: 3.5365  last_time: 3.5876  data_time: 0.1950  last_data_time: 0.1646   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:50:24 d2.utils.events]:  eta: 1 day, 0:15:15  iter: 5539  total_loss: 0.3407  loss_cls: 0.06876  loss_box_reg: 0.2698  loss_rpn_cls: 0.0003712  loss_rpn_loc: 0.004798    time: 3.5435  last_time: 3.5506  data_time: 0.1896  last_data_time: 0.1672   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:51:36 d2.utils.events]:  eta: 1 day, 0:15:34  iter: 5559  total_loss: 0.3382  loss_cls: 0.06552  loss_box_reg: 0.2678  loss_rpn_cls: 0.0002514  loss_rpn_loc: 0.005252    time: 3.5492  last_time: 3.5777  data_time: 0.1897  last_data_time: 0.1695   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:52:48 d2.utils.events]:  eta: 1 day, 0:14:23  iter: 5579  total_loss: 0.363  loss_cls: 0.07462  loss_box_reg: 0.2812  loss_rpn_cls: 0.0008196  loss_rpn_loc: 0.006146    time: 3.5539  last_time: 3.5699  data_time: 0.2092  last_data_time: 0.1594   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:54:06 d2.data.datasets.coco]: Loaded 355 images in COCO format from /content/SwimmingXDrowning-3/valid/_annotations.coco.json\n",
            "[04/24 07:54:06 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "|  Drowning  | 185          |  Swimming  | 460          |\n",
            "|            |              |            |              |\n",
            "|   total    | 645          |            |              |\n",
            "[04/24 07:54:06 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[04/24 07:54:06 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 07:54:06 d2.data.common]: Serializing 355 elements to byte tensors and concatenating them all ...\n",
            "[04/24 07:54:06 d2.data.common]: Serialized dataset takes 0.11 MiB\n",
            "WARNING [04/24 07:54:06 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[04/24 07:54:06 d2.evaluation.evaluator]: Start inference on 355 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 07:54:08 d2.evaluation.evaluator]: Inference done 11/355. Dataloading: 0.0010 s/iter. Inference: 0.1075 s/iter. Eval: 0.0003 s/iter. Total: 0.1087 s/iter. ETA=0:00:37\n",
            "[04/24 07:54:13 d2.evaluation.evaluator]: Inference done 56/355. Dataloading: 0.0022 s/iter. Inference: 0.1086 s/iter. Eval: 0.0003 s/iter. Total: 0.1112 s/iter. ETA=0:00:33\n",
            "[04/24 07:54:18 d2.evaluation.evaluator]: Inference done 101/355. Dataloading: 0.0022 s/iter. Inference: 0.1092 s/iter. Eval: 0.0003 s/iter. Total: 0.1119 s/iter. ETA=0:00:28\n",
            "[04/24 07:54:23 d2.evaluation.evaluator]: Inference done 146/355. Dataloading: 0.0020 s/iter. Inference: 0.1095 s/iter. Eval: 0.0003 s/iter. Total: 0.1119 s/iter. ETA=0:00:23\n",
            "[04/24 07:54:28 d2.evaluation.evaluator]: Inference done 190/355. Dataloading: 0.0022 s/iter. Inference: 0.1101 s/iter. Eval: 0.0003 s/iter. Total: 0.1127 s/iter. ETA=0:00:18\n",
            "[04/24 07:54:33 d2.evaluation.evaluator]: Inference done 235/355. Dataloading: 0.0021 s/iter. Inference: 0.1101 s/iter. Eval: 0.0003 s/iter. Total: 0.1126 s/iter. ETA=0:00:13\n",
            "[04/24 07:54:38 d2.evaluation.evaluator]: Inference done 280/355. Dataloading: 0.0021 s/iter. Inference: 0.1101 s/iter. Eval: 0.0003 s/iter. Total: 0.1125 s/iter. ETA=0:00:08\n",
            "[04/24 07:54:43 d2.evaluation.evaluator]: Inference done 324/355. Dataloading: 0.0022 s/iter. Inference: 0.1101 s/iter. Eval: 0.0003 s/iter. Total: 0.1127 s/iter. ETA=0:00:03\n",
            "[04/24 07:54:46 d2.evaluation.evaluator]: Total inference time: 0:00:39.475006 (0.112786 s / iter per device, on 1 devices)\n",
            "[04/24 07:54:46 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:38 (0.109964 s / iter per device, on 1 devices)\n",
            "[04/24 07:54:46 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 07:54:46 d2.evaluation.coco_evaluation]: Saving results to ./output_detection/inference/coco_instances_results.json\n",
            "[04/24 07:54:46 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[04/24 07:54:46 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[04/24 07:54:47 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.04 seconds.\n",
            "[04/24 07:54:47 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[04/24 07:54:47 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.352\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.124\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.535\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.537\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.530\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503\n",
            "[04/24 07:54:47 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 44.679 | 89.386 | 35.231 | 12.377 | 44.364 | 41.217 |\n",
            "[04/24 07:54:47 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| Drowning   | 47.534 | Swimming   | 41.825 |\n",
            "[04/24 07:54:47 d2.engine.defaults]: Evaluation results for swim_drown_val in csv format:\n",
            "[04/24 07:54:47 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 07:54:47 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 07:54:47 d2.evaluation.testing]: copypaste: 44.6794,89.3864,35.2312,12.3774,44.3635,41.2173\n",
            "[04/24 07:54:47 d2.utils.events]:  eta: 1 day, 0:11:57  iter: 5599  total_loss: 0.3618  loss_cls: 0.06807  loss_box_reg: 0.2846  loss_rpn_cls: 0.0007253  loss_rpn_loc: 0.005791    time: 3.5508  last_time: 3.5547  data_time: 0.1909  last_data_time: 0.1410   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:55:58 d2.utils.events]:  eta: 1 day, 0:10:59  iter: 5619  total_loss: 0.3489  loss_cls: 0.06462  loss_box_reg: 0.2739  loss_rpn_cls: 0.0004138  loss_rpn_loc: 0.005067    time: 3.5528  last_time: 3.5747  data_time: 0.1879  last_data_time: 0.1729   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:57:09 d2.utils.events]:  eta: 1 day, 0:09:38  iter: 5639  total_loss: 0.3642  loss_cls: 0.07135  loss_box_reg: 0.2872  loss_rpn_cls: 0.0004734  loss_rpn_loc: 0.005906    time: 3.5529  last_time: 3.6820  data_time: 0.2040  last_data_time: 0.2203   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:58:20 d2.utils.events]:  eta: 1 day, 0:09:12  iter: 5659  total_loss: 0.3465  loss_cls: 0.06528  loss_box_reg: 0.2757  loss_rpn_cls: 0.000219  loss_rpn_loc: 0.004547    time: 3.5537  last_time: 3.5723  data_time: 0.1833  last_data_time: 0.1802   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 07:59:31 d2.utils.events]:  eta: 1 day, 0:08:01  iter: 5679  total_loss: 0.3419  loss_cls: 0.0678  loss_box_reg: 0.2688  loss_rpn_cls: 0.0003408  loss_rpn_loc: 0.00493    time: 3.5504  last_time: 3.5841  data_time: 0.2009  last_data_time: 0.1817   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:00:42 d2.utils.events]:  eta: 1 day, 0:06:49  iter: 5699  total_loss: 0.3639  loss_cls: 0.06903  loss_box_reg: 0.2825  loss_rpn_cls: 0.0007103  loss_rpn_loc: 0.005051    time: 3.5523  last_time: 3.5690  data_time: 0.1925  last_data_time: 0.1872   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:01:54 d2.utils.events]:  eta: 1 day, 0:05:55  iter: 5719  total_loss: 0.3528  loss_cls: 0.07083  loss_box_reg: 0.2798  loss_rpn_cls: 0.0004705  loss_rpn_loc: 0.005161    time: 3.5560  last_time: 3.5572  data_time: 0.1970  last_data_time: 0.1570   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:03:06 d2.utils.events]:  eta: 1 day, 0:05:01  iter: 5739  total_loss: 0.354  loss_cls: 0.07178  loss_box_reg: 0.2761  loss_rpn_cls: 0.0004039  loss_rpn_loc: 0.005469    time: 3.5576  last_time: 3.6190  data_time: 0.1931  last_data_time: 0.2005   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:04:18 d2.utils.events]:  eta: 1 day, 0:04:17  iter: 5759  total_loss: 0.3467  loss_cls: 0.06546  loss_box_reg: 0.2729  loss_rpn_cls: 0.0008643  loss_rpn_loc: 0.005792    time: 3.5611  last_time: 3.5841  data_time: 0.2097  last_data_time: 0.1802   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:05:30 d2.utils.events]:  eta: 1 day, 0:03:06  iter: 5779  total_loss: 0.3286  loss_cls: 0.05967  loss_box_reg: 0.2666  loss_rpn_cls: 0.0002413  loss_rpn_loc: 0.004717    time: 3.5610  last_time: 3.6784  data_time: 0.1995  last_data_time: 0.2807   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:06:42 d2.data.datasets.coco]: Loaded 355 images in COCO format from /content/SwimmingXDrowning-3/valid/_annotations.coco.json\n",
            "[04/24 08:06:42 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[04/24 08:06:42 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 08:06:42 d2.data.common]: Serializing 355 elements to byte tensors and concatenating them all ...\n",
            "[04/24 08:06:42 d2.data.common]: Serialized dataset takes 0.11 MiB\n",
            "WARNING [04/24 08:06:42 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[04/24 08:06:42 d2.evaluation.evaluator]: Start inference on 355 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 08:06:44 d2.evaluation.evaluator]: Inference done 11/355. Dataloading: 0.0091 s/iter. Inference: 0.1071 s/iter. Eval: 0.0003 s/iter. Total: 0.1166 s/iter. ETA=0:00:40\n",
            "[04/24 08:06:49 d2.evaluation.evaluator]: Inference done 56/355. Dataloading: 0.0040 s/iter. Inference: 0.1083 s/iter. Eval: 0.0003 s/iter. Total: 0.1127 s/iter. ETA=0:00:33\n",
            "[04/24 08:06:54 d2.evaluation.evaluator]: Inference done 101/355. Dataloading: 0.0030 s/iter. Inference: 0.1087 s/iter. Eval: 0.0003 s/iter. Total: 0.1120 s/iter. ETA=0:00:28\n",
            "[04/24 08:06:59 d2.evaluation.evaluator]: Inference done 143/355. Dataloading: 0.0035 s/iter. Inference: 0.1109 s/iter. Eval: 0.0004 s/iter. Total: 0.1149 s/iter. ETA=0:00:24\n",
            "[04/24 08:07:05 d2.evaluation.evaluator]: Inference done 188/355. Dataloading: 0.0032 s/iter. Inference: 0.1105 s/iter. Eval: 0.0003 s/iter. Total: 0.1141 s/iter. ETA=0:00:19\n",
            "[04/24 08:07:10 d2.evaluation.evaluator]: Inference done 233/355. Dataloading: 0.0029 s/iter. Inference: 0.1102 s/iter. Eval: 0.0003 s/iter. Total: 0.1136 s/iter. ETA=0:00:13\n",
            "[04/24 08:07:15 d2.evaluation.evaluator]: Inference done 277/355. Dataloading: 0.0032 s/iter. Inference: 0.1101 s/iter. Eval: 0.0003 s/iter. Total: 0.1137 s/iter. ETA=0:00:08\n",
            "[04/24 08:07:20 d2.evaluation.evaluator]: Inference done 323/355. Dataloading: 0.0030 s/iter. Inference: 0.1099 s/iter. Eval: 0.0003 s/iter. Total: 0.1133 s/iter. ETA=0:00:03\n",
            "[04/24 08:07:23 d2.evaluation.evaluator]: Total inference time: 0:00:39.648410 (0.113281 s / iter per device, on 1 devices)\n",
            "[04/24 08:07:23 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:38 (0.109807 s / iter per device, on 1 devices)\n",
            "[04/24 08:07:23 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 08:07:23 d2.evaluation.coco_evaluation]: Saving results to ./output_detection/inference/coco_instances_results.json\n",
            "[04/24 08:07:23 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[04/24 08:07:23 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[04/24 08:07:23 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
            "[04/24 08:07:23 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[04/24 08:07:23 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.904\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.376\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.160\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.449\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.534\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.537\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.528\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "[04/24 08:07:23 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 45.756 | 90.352 | 37.614 | 15.958 | 44.857 | 42.116 |\n",
            "[04/24 08:07:23 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| Drowning   | 49.236 | Swimming   | 42.275 |\n",
            "[04/24 08:07:23 d2.engine.defaults]: Evaluation results for swim_drown_val in csv format:\n",
            "[04/24 08:07:23 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 08:07:23 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 08:07:23 d2.evaluation.testing]: copypaste: 45.7556,90.3522,37.6144,15.9579,44.8565,42.1157\n",
            "[04/24 08:07:23 d2.utils.events]:  eta: 1 day, 0:01:51  iter: 5799  total_loss: 0.3539  loss_cls: 0.06805  loss_box_reg: 0.2735  loss_rpn_cls: 0.0007931  loss_rpn_loc: 0.005408    time: 3.5602  last_time: 3.5466  data_time: 0.1859  last_data_time: 0.1412   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:08:34 d2.utils.events]:  eta: 1 day, 0:00:27  iter: 5819  total_loss: 0.3579  loss_cls: 0.06599  loss_box_reg: 0.2728  loss_rpn_cls: 0.0007552  loss_rpn_loc: 0.006019    time: 3.5595  last_time: 3.6880  data_time: 0.2047  last_data_time: 0.2589   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:09:45 d2.utils.events]:  eta: 23:59:16  iter: 5839  total_loss: 0.3531  loss_cls: 0.06795  loss_box_reg: 0.2761  loss_rpn_cls: 0.0004855  loss_rpn_loc: 0.005563    time: 3.5589  last_time: 3.5657  data_time: 0.1990  last_data_time: 0.1664   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:10:56 d2.utils.events]:  eta: 23:57:52  iter: 5859  total_loss: 0.3487  loss_cls: 0.0659  loss_box_reg: 0.2739  loss_rpn_cls: 0.0006719  loss_rpn_loc: 0.005611    time: 3.5590  last_time: 3.5964  data_time: 0.1905  last_data_time: 0.1834   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:12:09 d2.utils.events]:  eta: 23:57:02  iter: 5879  total_loss: 0.3364  loss_cls: 0.06265  loss_box_reg: 0.2658  loss_rpn_cls: 0.0003933  loss_rpn_loc: 0.004807    time: 3.5613  last_time: 3.5745  data_time: 0.1985  last_data_time: 0.1610   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:13:21 d2.utils.events]:  eta: 23:56:03  iter: 5899  total_loss: 0.3324  loss_cls: 0.06522  loss_box_reg: 0.26  loss_rpn_cls: 0.0003725  loss_rpn_loc: 0.005389    time: 3.5626  last_time: 3.5807  data_time: 0.2072  last_data_time: 0.1663   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:14:32 d2.utils.events]:  eta: 23:54:45  iter: 5919  total_loss: 0.3465  loss_cls: 0.06766  loss_box_reg: 0.2721  loss_rpn_cls: 0.0008176  loss_rpn_loc: 0.005484    time: 3.5623  last_time: 3.5730  data_time: 0.1908  last_data_time: 0.2061   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:15:43 d2.utils.events]:  eta: 23:53:32  iter: 5939  total_loss: 0.3574  loss_cls: 0.07446  loss_box_reg: 0.2729  loss_rpn_cls: 0.000481  loss_rpn_loc: 0.006094    time: 3.5631  last_time: 3.1623  data_time: 0.1961  last_data_time: 0.1290   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:16:56 d2.utils.events]:  eta: 23:52:16  iter: 5959  total_loss: 0.3338  loss_cls: 0.06583  loss_box_reg: 0.267  loss_rpn_cls: 0.0003033  loss_rpn_loc: 0.005197    time: 3.5648  last_time: 3.5468  data_time: 0.2022  last_data_time: 0.1540   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:18:08 d2.utils.events]:  eta: 23:51:06  iter: 5979  total_loss: 0.3421  loss_cls: 0.06579  loss_box_reg: 0.2606  loss_rpn_cls: 0.0004228  loss_rpn_loc: 0.005013    time: 3.5662  last_time: 3.5346  data_time: 0.2088  last_data_time: 0.1635   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:19:24 d2.data.datasets.coco]: Loaded 355 images in COCO format from /content/SwimmingXDrowning-3/valid/_annotations.coco.json\n",
            "[04/24 08:19:24 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[04/24 08:19:24 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 08:19:24 d2.data.common]: Serializing 355 elements to byte tensors and concatenating them all ...\n",
            "[04/24 08:19:24 d2.data.common]: Serialized dataset takes 0.11 MiB\n",
            "WARNING [04/24 08:19:24 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[04/24 08:19:24 d2.evaluation.evaluator]: Start inference on 355 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 08:19:26 d2.evaluation.evaluator]: Inference done 11/355. Dataloading: 0.0009 s/iter. Inference: 0.1077 s/iter. Eval: 0.0002 s/iter. Total: 0.1089 s/iter. ETA=0:00:37\n",
            "[04/24 08:19:31 d2.evaluation.evaluator]: Inference done 55/355. Dataloading: 0.0047 s/iter. Inference: 0.1087 s/iter. Eval: 0.0004 s/iter. Total: 0.1138 s/iter. ETA=0:00:34\n",
            "[04/24 08:19:36 d2.evaluation.evaluator]: Inference done 100/355. Dataloading: 0.0035 s/iter. Inference: 0.1091 s/iter. Eval: 0.0004 s/iter. Total: 0.1132 s/iter. ETA=0:00:28\n",
            "[04/24 08:19:41 d2.evaluation.evaluator]: Inference done 145/355. Dataloading: 0.0034 s/iter. Inference: 0.1093 s/iter. Eval: 0.0004 s/iter. Total: 0.1133 s/iter. ETA=0:00:23\n",
            "[04/24 08:19:46 d2.evaluation.evaluator]: Inference done 190/355. Dataloading: 0.0035 s/iter. Inference: 0.1093 s/iter. Eval: 0.0003 s/iter. Total: 0.1133 s/iter. ETA=0:00:18\n",
            "[04/24 08:19:51 d2.evaluation.evaluator]: Inference done 235/355. Dataloading: 0.0031 s/iter. Inference: 0.1094 s/iter. Eval: 0.0003 s/iter. Total: 0.1130 s/iter. ETA=0:00:13\n",
            "[04/24 08:19:56 d2.evaluation.evaluator]: Inference done 279/355. Dataloading: 0.0035 s/iter. Inference: 0.1094 s/iter. Eval: 0.0003 s/iter. Total: 0.1133 s/iter. ETA=0:00:08\n",
            "[04/24 08:20:01 d2.evaluation.evaluator]: Inference done 324/355. Dataloading: 0.0033 s/iter. Inference: 0.1094 s/iter. Eval: 0.0003 s/iter. Total: 0.1131 s/iter. ETA=0:00:03\n",
            "[04/24 08:20:05 d2.evaluation.evaluator]: Total inference time: 0:00:39.618613 (0.113196 s / iter per device, on 1 devices)\n",
            "[04/24 08:20:05 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:38 (0.109397 s / iter per device, on 1 devices)\n",
            "[04/24 08:20:05 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 08:20:05 d2.evaluation.coco_evaluation]: Saving results to ./output_detection/inference/coco_instances_results.json\n",
            "[04/24 08:20:05 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[04/24 08:20:05 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[04/24 08:20:05 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
            "[04/24 08:20:05 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[04/24 08:20:05 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.915\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.390\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.127\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.458\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.420\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.399\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.541\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.544\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.180\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.545\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
            "[04/24 08:20:05 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 46.063 | 91.517 | 38.992 | 12.682 | 45.825 | 42.047 |\n",
            "[04/24 08:20:05 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| Drowning   | 49.705 | Swimming   | 42.420 |\n",
            "[04/24 08:20:05 d2.engine.defaults]: Evaluation results for swim_drown_val in csv format:\n",
            "[04/24 08:20:05 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 08:20:05 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 08:20:05 d2.evaluation.testing]: copypaste: 46.0628,91.5169,38.9922,12.6815,45.8251,42.0465\n",
            "[04/24 08:20:05 d2.utils.events]:  eta: 23:49:58  iter: 5999  total_loss: 0.3394  loss_cls: 0.06669  loss_box_reg: 0.2609  loss_rpn_cls: 0.0003465  loss_rpn_loc: 0.005309    time: 3.5657  last_time: 3.6812  data_time: 0.2105  last_data_time: 0.2582   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:21:16 d2.utils.events]:  eta: 23:48:43  iter: 6019  total_loss: 0.3548  loss_cls: 0.06938  loss_box_reg: 0.2762  loss_rpn_cls: 0.0007103  loss_rpn_loc: 0.005605    time: 3.5656  last_time: 3.7004  data_time: 0.2177  last_data_time: 0.2760   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:22:28 d2.utils.events]:  eta: 23:47:30  iter: 6039  total_loss: 0.3416  loss_cls: 0.071  loss_box_reg: 0.2698  loss_rpn_cls: 0.0005364  loss_rpn_loc: 0.006083    time: 3.5666  last_time: 3.5966  data_time: 0.1919  last_data_time: 0.1743   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:23:39 d2.utils.events]:  eta: 23:46:01  iter: 6059  total_loss: 0.3178  loss_cls: 0.06679  loss_box_reg: 0.2454  loss_rpn_cls: 0.0003866  loss_rpn_loc: 0.005345    time: 3.5657  last_time: 3.5777  data_time: 0.1877  last_data_time: 0.1404   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:24:51 d2.utils.events]:  eta: 23:44:50  iter: 6079  total_loss: 0.3439  loss_cls: 0.06534  loss_box_reg: 0.2782  loss_rpn_cls: 0.0003258  loss_rpn_loc: 0.005201    time: 3.5675  last_time: 3.5462  data_time: 0.2174  last_data_time: 0.1338   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:26:02 d2.utils.events]:  eta: 23:43:34  iter: 6099  total_loss: 0.337  loss_cls: 0.06008  loss_box_reg: 0.2651  loss_rpn_cls: 0.0007333  loss_rpn_loc: 0.004896    time: 3.5667  last_time: 3.5538  data_time: 0.1945  last_data_time: 0.1529   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:27:13 d2.utils.events]:  eta: 23:42:27  iter: 6119  total_loss: 0.3617  loss_cls: 0.0666  loss_box_reg: 0.2811  loss_rpn_cls: 0.0004282  loss_rpn_loc: 0.005612    time: 3.5665  last_time: 3.5742  data_time: 0.1980  last_data_time: 0.1796   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:28:25 d2.utils.events]:  eta: 23:41:20  iter: 6139  total_loss: 0.3166  loss_cls: 0.06616  loss_box_reg: 0.2495  loss_rpn_cls: 0.0006947  loss_rpn_loc: 0.004777    time: 3.5672  last_time: 3.5512  data_time: 0.2048  last_data_time: 0.1560   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:29:36 d2.utils.events]:  eta: 23:40:06  iter: 6159  total_loss: 0.3341  loss_cls: 0.06181  loss_box_reg: 0.2535  loss_rpn_cls: 0.0002653  loss_rpn_loc: 0.004872    time: 3.5665  last_time: 3.6694  data_time: 0.2097  last_data_time: 0.2610   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:30:48 d2.utils.events]:  eta: 23:39:01  iter: 6179  total_loss: 0.3237  loss_cls: 0.0616  loss_box_reg: 0.2556  loss_rpn_cls: 0.0002919  loss_rpn_loc: 0.005451    time: 3.5668  last_time: 3.5899  data_time: 0.1919  last_data_time: 0.1842   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:32:01 d2.data.datasets.coco]: Loaded 355 images in COCO format from /content/SwimmingXDrowning-3/valid/_annotations.coco.json\n",
            "[04/24 08:32:01 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[04/24 08:32:01 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 08:32:01 d2.data.common]: Serializing 355 elements to byte tensors and concatenating them all ...\n",
            "[04/24 08:32:01 d2.data.common]: Serialized dataset takes 0.11 MiB\n",
            "WARNING [04/24 08:32:01 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[04/24 08:32:01 d2.evaluation.evaluator]: Start inference on 355 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 08:32:03 d2.evaluation.evaluator]: Inference done 11/355. Dataloading: 0.0010 s/iter. Inference: 0.1092 s/iter. Eval: 0.0002 s/iter. Total: 0.1105 s/iter. ETA=0:00:38\n",
            "[04/24 08:32:08 d2.evaluation.evaluator]: Inference done 57/355. Dataloading: 0.0017 s/iter. Inference: 0.1085 s/iter. Eval: 0.0002 s/iter. Total: 0.1106 s/iter. ETA=0:00:32\n",
            "[04/24 08:32:13 d2.evaluation.evaluator]: Inference done 98/355. Dataloading: 0.0034 s/iter. Inference: 0.1118 s/iter. Eval: 0.0003 s/iter. Total: 0.1157 s/iter. ETA=0:00:29\n",
            "[04/24 08:32:18 d2.evaluation.evaluator]: Inference done 144/355. Dataloading: 0.0028 s/iter. Inference: 0.1108 s/iter. Eval: 0.0003 s/iter. Total: 0.1141 s/iter. ETA=0:00:24\n",
            "[04/24 08:32:23 d2.evaluation.evaluator]: Inference done 189/355. Dataloading: 0.0030 s/iter. Inference: 0.1102 s/iter. Eval: 0.0003 s/iter. Total: 0.1135 s/iter. ETA=0:00:18\n",
            "[04/24 08:32:28 d2.evaluation.evaluator]: Inference done 234/355. Dataloading: 0.0030 s/iter. Inference: 0.1100 s/iter. Eval: 0.0003 s/iter. Total: 0.1133 s/iter. ETA=0:00:13\n",
            "[04/24 08:32:33 d2.evaluation.evaluator]: Inference done 280/355. Dataloading: 0.0028 s/iter. Inference: 0.1097 s/iter. Eval: 0.0003 s/iter. Total: 0.1129 s/iter. ETA=0:00:08\n",
            "[04/24 08:32:38 d2.evaluation.evaluator]: Inference done 324/355. Dataloading: 0.0030 s/iter. Inference: 0.1097 s/iter. Eval: 0.0003 s/iter. Total: 0.1130 s/iter. ETA=0:00:03\n",
            "[04/24 08:32:41 d2.evaluation.evaluator]: Total inference time: 0:00:39.588713 (0.113111 s / iter per device, on 1 devices)\n",
            "[04/24 08:32:41 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:38 (0.109559 s / iter per device, on 1 devices)\n",
            "[04/24 08:32:42 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 08:32:42 d2.evaluation.coco_evaluation]: Saving results to ./output_detection/inference/coco_instances_results.json\n",
            "[04/24 08:32:42 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[04/24 08:32:42 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[04/24 08:32:42 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
            "[04/24 08:32:42 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[04/24 08:32:42 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.910\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.361\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.120\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.454\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.530\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.533\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.549\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491\n",
            "[04/24 08:32:42 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 44.696 | 91.007 | 36.101 | 11.978 | 45.370 | 40.754 |\n",
            "[04/24 08:32:42 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| Drowning   | 47.453 | Swimming   | 41.939 |\n",
            "[04/24 08:32:42 d2.engine.defaults]: Evaluation results for swim_drown_val in csv format:\n",
            "[04/24 08:32:42 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 08:32:42 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 08:32:42 d2.evaluation.testing]: copypaste: 44.6955,91.0065,36.1008,11.9776,45.3700,40.7540\n",
            "[04/24 08:32:42 d2.utils.events]:  eta: 23:37:55  iter: 6199  total_loss: 0.343  loss_cls: 0.06196  loss_box_reg: 0.2716  loss_rpn_cls: 0.0003057  loss_rpn_loc: 0.005292    time: 3.5671  last_time: 3.7391  data_time: 0.1853  last_data_time: 0.2678   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:33:53 d2.utils.events]:  eta: 23:36:32  iter: 6219  total_loss: 0.3414  loss_cls: 0.07399  loss_box_reg: 0.2619  loss_rpn_cls: 0.0008114  loss_rpn_loc: 0.005503    time: 3.5671  last_time: 3.5729  data_time: 0.1869  last_data_time: 0.1696   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:35:04 d2.utils.events]:  eta: 23:35:18  iter: 6239  total_loss: 0.3347  loss_cls: 0.07107  loss_box_reg: 0.2577  loss_rpn_cls: 0.0002934  loss_rpn_loc: 0.005334    time: 3.5667  last_time: 3.8413  data_time: 0.1945  last_data_time: 0.3921   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:36:16 d2.utils.events]:  eta: 23:34:12  iter: 6259  total_loss: 0.3067  loss_cls: 0.06209  loss_box_reg: 0.2408  loss_rpn_cls: 0.0004768  loss_rpn_loc: 0.004775    time: 3.5675  last_time: 3.6053  data_time: 0.1931  last_data_time: 0.1970   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:37:28 d2.utils.events]:  eta: 23:33:14  iter: 6279  total_loss: 0.331  loss_cls: 0.06329  loss_box_reg: 0.267  loss_rpn_cls: 0.0006403  loss_rpn_loc: 0.005257    time: 3.5679  last_time: 3.7533  data_time: 0.2023  last_data_time: 0.3001   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:38:40 d2.utils.events]:  eta: 23:32:05  iter: 6299  total_loss: 0.3349  loss_cls: 0.06449  loss_box_reg: 0.2621  loss_rpn_cls: 0.0007944  loss_rpn_loc: 0.005678    time: 3.5686  last_time: 3.5380  data_time: 0.1889  last_data_time: 0.1602   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:39:51 d2.utils.events]:  eta: 23:30:53  iter: 6319  total_loss: 0.3338  loss_cls: 0.06611  loss_box_reg: 0.255  loss_rpn_cls: 0.0005279  loss_rpn_loc: 0.005109    time: 3.5686  last_time: 3.5979  data_time: 0.1932  last_data_time: 0.1651   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:41:03 d2.utils.events]:  eta: 23:29:49  iter: 6339  total_loss: 0.3355  loss_cls: 0.05988  loss_box_reg: 0.2701  loss_rpn_cls: 0.0007948  loss_rpn_loc: 0.005031    time: 3.5697  last_time: 3.5654  data_time: 0.2092  last_data_time: 0.1523   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:42:15 d2.utils.events]:  eta: 23:28:50  iter: 6359  total_loss: 0.322  loss_cls: 0.0599  loss_box_reg: 0.2602  loss_rpn_cls: 0.0004261  loss_rpn_loc: 0.004284    time: 3.5700  last_time: 3.6052  data_time: 0.1914  last_data_time: 0.1654   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:43:27 d2.utils.events]:  eta: 23:27:31  iter: 6379  total_loss: 0.3409  loss_cls: 0.06722  loss_box_reg: 0.262  loss_rpn_cls: 0.0008312  loss_rpn_loc: 0.00551    time: 3.5704  last_time: 3.5577  data_time: 0.1977  last_data_time: 0.1636   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:44:44 d2.data.datasets.coco]: Loaded 355 images in COCO format from /content/SwimmingXDrowning-3/valid/_annotations.coco.json\n",
            "[04/24 08:44:44 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[04/24 08:44:44 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 08:44:44 d2.data.common]: Serializing 355 elements to byte tensors and concatenating them all ...\n",
            "[04/24 08:44:44 d2.data.common]: Serialized dataset takes 0.11 MiB\n",
            "WARNING [04/24 08:44:44 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[04/24 08:44:44 d2.evaluation.evaluator]: Start inference on 355 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 08:44:46 d2.evaluation.evaluator]: Inference done 11/355. Dataloading: 0.0010 s/iter. Inference: 0.1079 s/iter. Eval: 0.0003 s/iter. Total: 0.1092 s/iter. ETA=0:00:37\n",
            "[04/24 08:44:51 d2.evaluation.evaluator]: Inference done 57/355. Dataloading: 0.0021 s/iter. Inference: 0.1074 s/iter. Eval: 0.0003 s/iter. Total: 0.1099 s/iter. ETA=0:00:32\n",
            "[04/24 08:44:56 d2.evaluation.evaluator]: Inference done 97/355. Dataloading: 0.0041 s/iter. Inference: 0.1125 s/iter. Eval: 0.0003 s/iter. Total: 0.1172 s/iter. ETA=0:00:30\n",
            "[04/24 08:45:01 d2.evaluation.evaluator]: Inference done 142/355. Dataloading: 0.0033 s/iter. Inference: 0.1115 s/iter. Eval: 0.0003 s/iter. Total: 0.1152 s/iter. ETA=0:00:24\n",
            "[04/24 08:45:06 d2.evaluation.evaluator]: Inference done 187/355. Dataloading: 0.0034 s/iter. Inference: 0.1110 s/iter. Eval: 0.0003 s/iter. Total: 0.1148 s/iter. ETA=0:00:19\n",
            "[04/24 08:45:11 d2.evaluation.evaluator]: Inference done 231/355. Dataloading: 0.0033 s/iter. Inference: 0.1108 s/iter. Eval: 0.0003 s/iter. Total: 0.1146 s/iter. ETA=0:00:14\n",
            "[04/24 08:45:16 d2.evaluation.evaluator]: Inference done 276/355. Dataloading: 0.0030 s/iter. Inference: 0.1106 s/iter. Eval: 0.0003 s/iter. Total: 0.1140 s/iter. ETA=0:00:09\n",
            "[04/24 08:45:21 d2.evaluation.evaluator]: Inference done 320/355. Dataloading: 0.0032 s/iter. Inference: 0.1105 s/iter. Eval: 0.0003 s/iter. Total: 0.1141 s/iter. ETA=0:00:03\n",
            "[04/24 08:45:25 d2.evaluation.evaluator]: Total inference time: 0:00:39.908793 (0.114025 s / iter per device, on 1 devices)\n",
            "[04/24 08:45:25 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:38 (0.110334 s / iter per device, on 1 devices)\n",
            "[04/24 08:45:25 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 08:45:25 d2.evaluation.coco_evaluation]: Saving results to ./output_detection/inference/coco_instances_results.json\n",
            "[04/24 08:45:25 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[04/24 08:45:25 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[04/24 08:45:25 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
            "[04/24 08:45:25 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[04/24 08:45:25 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.442\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.908\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.339\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.145\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.422\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.531\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.533\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.535\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502\n",
            "[04/24 08:45:25 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 44.235 | 90.752 | 33.880 | 14.536 | 43.350 | 42.150 |\n",
            "[04/24 08:45:25 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| Drowning   | 46.466 | Swimming   | 42.004 |\n",
            "[04/24 08:45:25 d2.engine.defaults]: Evaluation results for swim_drown_val in csv format:\n",
            "[04/24 08:45:25 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 08:45:25 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 08:45:25 d2.evaluation.testing]: copypaste: 44.2351,90.7522,33.8798,14.5359,43.3503,42.1501\n",
            "[04/24 08:45:25 d2.utils.events]:  eta: 23:26:27  iter: 6399  total_loss: 0.3332  loss_cls: 0.06703  loss_box_reg: 0.2645  loss_rpn_cls: 0.0005536  loss_rpn_loc: 0.005673    time: 3.5707  last_time: 3.5192  data_time: 0.2025  last_data_time: 0.1654   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:46:37 d2.utils.events]:  eta: 23:25:34  iter: 6419  total_loss: 0.3416  loss_cls: 0.06367  loss_box_reg: 0.2586  loss_rpn_cls: 0.0005141  loss_rpn_loc: 0.005024    time: 3.5711  last_time: 3.5761  data_time: 0.1933  last_data_time: 0.1812   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:47:47 d2.utils.events]:  eta: 23:24:15  iter: 6439  total_loss: 0.3328  loss_cls: 0.06298  loss_box_reg: 0.2582  loss_rpn_cls: 0.0002764  loss_rpn_loc: 0.005286    time: 3.5700  last_time: 3.7396  data_time: 0.2033  last_data_time: 0.3034   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:48:59 d2.utils.events]:  eta: 23:23:03  iter: 6459  total_loss: 0.3193  loss_cls: 0.0583  loss_box_reg: 0.2557  loss_rpn_cls: 0.0007907  loss_rpn_loc: 0.00476    time: 3.5703  last_time: 3.6009  data_time: 0.2003  last_data_time: 0.1872   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:50:11 d2.utils.events]:  eta: 23:21:52  iter: 6479  total_loss: 0.343  loss_cls: 0.07298  loss_box_reg: 0.2689  loss_rpn_cls: 0.000348  loss_rpn_loc: 0.005593    time: 3.5705  last_time: 3.7402  data_time: 0.2063  last_data_time: 0.2599   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:51:22 d2.utils.events]:  eta: 23:20:41  iter: 6499  total_loss: 0.3073  loss_cls: 0.06268  loss_box_reg: 0.2394  loss_rpn_cls: 0.0006381  loss_rpn_loc: 0.00459    time: 3.5702  last_time: 3.5278  data_time: 0.2063  last_data_time: 0.1469   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:52:34 d2.utils.events]:  eta: 23:19:23  iter: 6519  total_loss: 0.3229  loss_cls: 0.06558  loss_box_reg: 0.2563  loss_rpn_cls: 0.0004035  loss_rpn_loc: 0.005278    time: 3.5706  last_time: 3.6384  data_time: 0.1967  last_data_time: 0.2039   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:53:45 d2.utils.events]:  eta: 23:18:22  iter: 6539  total_loss: 0.3373  loss_cls: 0.06861  loss_box_reg: 0.2634  loss_rpn_cls: 0.0005309  loss_rpn_loc: 0.00558    time: 3.5706  last_time: 3.5625  data_time: 0.2019  last_data_time: 0.1542   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:54:57 d2.utils.events]:  eta: 23:17:04  iter: 6559  total_loss: 0.3373  loss_cls: 0.06375  loss_box_reg: 0.2678  loss_rpn_cls: 0.000421  loss_rpn_loc: 0.005541    time: 3.5707  last_time: 3.6275  data_time: 0.1967  last_data_time: 0.1803   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:56:09 d2.utils.events]:  eta: 23:15:59  iter: 6579  total_loss: 0.3247  loss_cls: 0.0613  loss_box_reg: 0.2467  loss_rpn_cls: 0.000514  loss_rpn_loc: 0.005404    time: 3.5713  last_time: 3.5360  data_time: 0.1976  last_data_time: 0.1532   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:57:25 d2.data.datasets.coco]: Loaded 355 images in COCO format from /content/SwimmingXDrowning-3/valid/_annotations.coco.json\n",
            "[04/24 08:57:25 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[04/24 08:57:25 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[04/24 08:57:25 d2.data.common]: Serializing 355 elements to byte tensors and concatenating them all ...\n",
            "[04/24 08:57:25 d2.data.common]: Serialized dataset takes 0.11 MiB\n",
            "WARNING [04/24 08:57:25 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "[04/24 08:57:25 d2.evaluation.evaluator]: Start inference on 355 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04/24 08:57:26 d2.evaluation.evaluator]: Inference done 11/355. Dataloading: 0.0014 s/iter. Inference: 0.1081 s/iter. Eval: 0.0003 s/iter. Total: 0.1098 s/iter. ETA=0:00:37\n",
            "[04/24 08:57:31 d2.evaluation.evaluator]: Inference done 57/355. Dataloading: 0.0015 s/iter. Inference: 0.1078 s/iter. Eval: 0.0002 s/iter. Total: 0.1097 s/iter. ETA=0:00:32\n",
            "[04/24 08:57:36 d2.evaluation.evaluator]: Inference done 97/355. Dataloading: 0.0031 s/iter. Inference: 0.1131 s/iter. Eval: 0.0003 s/iter. Total: 0.1166 s/iter. ETA=0:00:30\n",
            "[04/24 08:57:41 d2.evaluation.evaluator]: Inference done 142/355. Dataloading: 0.0027 s/iter. Inference: 0.1121 s/iter. Eval: 0.0003 s/iter. Total: 0.1152 s/iter. ETA=0:00:24\n",
            "[04/24 08:57:46 d2.evaluation.evaluator]: Inference done 187/355. Dataloading: 0.0027 s/iter. Inference: 0.1114 s/iter. Eval: 0.0003 s/iter. Total: 0.1145 s/iter. ETA=0:00:19\n",
            "[04/24 08:57:51 d2.evaluation.evaluator]: Inference done 231/355. Dataloading: 0.0030 s/iter. Inference: 0.1110 s/iter. Eval: 0.0003 s/iter. Total: 0.1144 s/iter. ETA=0:00:14\n",
            "[04/24 08:57:56 d2.evaluation.evaluator]: Inference done 275/355. Dataloading: 0.0032 s/iter. Inference: 0.1107 s/iter. Eval: 0.0003 s/iter. Total: 0.1143 s/iter. ETA=0:00:09\n",
            "[04/24 08:58:02 d2.evaluation.evaluator]: Inference done 319/355. Dataloading: 0.0036 s/iter. Inference: 0.1105 s/iter. Eval: 0.0003 s/iter. Total: 0.1145 s/iter. ETA=0:00:04\n",
            "[04/24 08:58:06 d2.evaluation.evaluator]: Total inference time: 0:00:40.079500 (0.114513 s / iter per device, on 1 devices)\n",
            "[04/24 08:58:06 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:38 (0.110367 s / iter per device, on 1 devices)\n",
            "[04/24 08:58:06 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[04/24 08:58:06 d2.evaluation.coco_evaluation]: Saving results to ./output_detection/inference/coco_instances_results.json\n",
            "[04/24 08:58:06 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[04/24 08:58:06 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[04/24 08:58:06 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
            "[04/24 08:58:06 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[04/24 08:58:06 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.455\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.914\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.366\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.138\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.448\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.423\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.536\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.538\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.188\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.533\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "[04/24 08:58:06 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 45.464 | 91.368 | 36.575 | 13.802 | 44.834 | 42.269 |\n",
            "[04/24 08:58:06 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|\n",
            "| Drowning   | 48.677 | Swimming   | 42.252 |\n",
            "[04/24 08:58:06 d2.engine.defaults]: Evaluation results for swim_drown_val in csv format:\n",
            "[04/24 08:58:06 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[04/24 08:58:06 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[04/24 08:58:06 d2.evaluation.testing]: copypaste: 45.4644,91.3678,36.5749,13.8023,44.8338,42.2686\n",
            "[04/24 08:58:06 d2.utils.events]:  eta: 23:14:54  iter: 6599  total_loss: 0.3217  loss_cls: 0.06136  loss_box_reg: 0.2503  loss_rpn_cls: 0.0003206  loss_rpn_loc: 0.004676    time: 3.5708  last_time: 3.5403  data_time: 0.1955  last_data_time: 0.1335   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 08:59:17 d2.utils.events]:  eta: 23:13:42  iter: 6619  total_loss: 0.3321  loss_cls: 0.07046  loss_box_reg: 0.2625  loss_rpn_cls: 0.0008272  loss_rpn_loc: 0.005431    time: 3.5709  last_time: 3.6538  data_time: 0.2014  last_data_time: 0.2388   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 09:00:28 d2.utils.events]:  eta: 23:12:31  iter: 6639  total_loss: 0.328  loss_cls: 0.061  loss_box_reg: 0.2614  loss_rpn_cls: 0.0004525  loss_rpn_loc: 0.005099    time: 3.5705  last_time: 3.7743  data_time: 0.1975  last_data_time: 0.3485   lr: 0.00025  max_mem: 13149M\n",
            "[04/24 09:01:40 d2.utils.events]:  eta: 23:11:19  iter: 6659  total_loss: 0.3114  loss_cls: 0.05667  loss_box_reg: 0.2528  loss_rpn_cls: 0.0004224  loss_rpn_loc: 0.004695    time: 3.5708  last_time: 3.5547  data_time: 0.1959  last_data_time: 0.1507   lr: 0.00025  max_mem: 13149M\n"
          ]
        }
      ],
      "source": [
        "# --- Standard Libraries ---\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- PyTorch ---\n",
        "import torch\n",
        "\n",
        "# --- OpenCV ---\n",
        "import cv2 # Make sure OpenCV is installed (pip install opencv-python)\n",
        "\n",
        "# --- Detectron2 Core ---\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "# --- Setup Logger ---\n",
        "# Sets up Detectron2's default logger to show logs in the console\n",
        "setup_logger()\n",
        "\n",
        "# ==============================================================================\n",
        "#   1. Register Your Custom Datasets (COCO Format)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Define dataset names and paths ---\n",
        "# Choose unique names for your training and validation datasets\n",
        "train_dataset_name = \"swim_drown_train\"\n",
        "train_annotations_path = \"/content/SwimmingXDrowning-3/train/_annotations.coco.json\"\n",
        "train_images_dir = \"/content/SwimmingXDrowning-3/train/\"\n",
        "\n",
        "val_dataset_name = \"swim_drown_val\"\n",
        "val_annotations_path = \"/content/SwimmingXDrowning-3/valid/_annotations.coco.json\"\n",
        "val_images_dir = \"/content/SwimmingXDrowning-3/valid/\"\n",
        "\n",
        "# --- Register the datasets ---\n",
        "# This tells Detectron2 how to load your data.\n",
        "# The empty dictionary {} is for potential metadata overrides, usually not needed for COCO.\n",
        "# We add a check to prevent errors if this code block is run multiple times.\n",
        "print(\"Registering datasets...\")\n",
        "if train_dataset_name not in DatasetCatalog.list():\n",
        "    register_coco_instances(train_dataset_name, {}, train_annotations_path, train_images_dir)\n",
        "    print(f\"Registered '{train_dataset_name}'\")\n",
        "    # Optional: Set metadata like class names here if not in COCO json, but COCO format usually handles this.\n",
        "    # MetadataCatalog.get(train_dataset_name).thing_classes = [\"swimming\", \"drowning\"]\n",
        "else:\n",
        "    print(f\"'{train_dataset_name}' already registered.\")\n",
        "\n",
        "if val_dataset_name not in DatasetCatalog.list():\n",
        "    register_coco_instances(val_dataset_name, {}, val_annotations_path, val_images_dir)\n",
        "    print(f\"Registered '{val_dataset_name}'\")\n",
        "    # Optional: Set metadata like class names here\n",
        "    # MetadataCatalog.get(val_dataset_name).thing_classes = [\"swimming\", \"drowning\"]\n",
        "else:\n",
        "    print(f\"'{val_dataset_name}' already registered.\")\n",
        "print(\"Dataset registration finished.\")\n",
        "\n",
        "# --- Optional: Verify Registration & Get Metadata ---\n",
        "# Uncomment this block to inspect the loaded metadata after registration\n",
        "# try:\n",
        "#     print(\"\\n--- Verifying Metadata ---\")\n",
        "#     train_metadata = MetadataCatalog.get(train_dataset_name)\n",
        "#     print(f\"Training Metadata for '{train_dataset_name}':\")\n",
        "#     print(train_metadata)\n",
        "#     # Automatically determine the number of classes from registered metadata\n",
        "#     num_classes = len(train_metadata.thing_classes)\n",
        "#     print(f\"Detected {num_classes} classes: {train_metadata.thing_classes}\")\n",
        "# except KeyError:\n",
        "#     print(f\"Error: Could not retrieve metadata for '{train_dataset_name}'. Ensure registration was successful.\")\n",
        "#     num_classes = None # Fallback\n",
        "\n",
        "# ==============================================================================\n",
        "#   2. Configure the Model and Training Parameters\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Configuring Model ---\")\n",
        "cfg = get_cfg()\n",
        "\n",
        "# --- Choose a Model Architecture ---\n",
        "# Load configuration from Detectron2's model zoo.\n",
        "# Faster R-CNN with ResNet-101 backbone and FPN is a strong baseline.\n",
        "model_config_path = \"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_config_path))\n",
        "print(f\"Loaded base configuration from: {model_config_path}\")\n",
        "\n",
        "# --- Load Pre-trained Weights ---\n",
        "# Start fine-tuning from weights pre-trained on the COCO dataset.\n",
        "# Using the same identifier ensures config and weights match.\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config_path)\n",
        "print(f\"Set model weights URL: {cfg.MODEL.WEIGHTS}\")\n",
        "\n",
        "# --- Set Dataset Information ---\n",
        "cfg.DATASETS.TRAIN = (train_dataset_name,)\n",
        "cfg.DATASETS.TEST = (val_dataset_name,) # Use validation set for evaluation during training\n",
        "cfg.DATALOADER.NUM_WORKERS = 4 # Number of CPU cores for data loading. Adjust based on your system. Lower if you face dataloader issues.\n",
        "\n",
        "# --- ** CRITICAL: Set Number of Classes ** ---\n",
        "# This MUST match the number of foreground object categories in your dataset.\n",
        "# Example: If your classes are 'swimming' and 'drowning', NUM_CLASSES = 2.\n",
        "# Check your _annotations.coco.json file under \"categories\".\n",
        "# Detectron2 automatically handles the background class.\n",
        "NUM_CLASSES_IN_YOUR_DATASET = 2 # <--- !!! VERIFY AND CHANGE THIS VALUE !!!\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES_IN_YOUR_DATASET\n",
        "print(f\"Set number of foreground classes: {cfg.MODEL.ROI_HEADS.NUM_CLASSES}\")\n",
        "\n",
        "# --- Fine-tuning Hyperparameters ---\n",
        "# Adjust these based on your dataset size, GPU memory, and desired training time.\n",
        "\n",
        "# Batch Size: Number of images processed in one iteration.\n",
        "# **ADJUST THIS BASED ON YOUR GPU MEMORY!** Start low (e.g., 2 or 4) and increase if possible.\n",
        "# 14 might work on GPUs with >=16GB VRAM, but could cause OOM on smaller GPUs.\n",
        "cfg.SOLVER.IMS_PER_BATCH = 14\n",
        "print(f\"Set Images Per Batch (Batch Size): {cfg.SOLVER.IMS_PER_BATCH}\")\n",
        "\n",
        "# Base Learning Rate: Starting learning rate. 0.00025 is often a good starting point for fine-tuning.\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "\n",
        "# Max Iterations: Total number of training iterations.\n",
        "# COCO's 3x schedule (~270k) is usually too long for fine-tuning.\n",
        "# 5000 is an example; adjust based on observing validation performance.\n",
        "cfg.SOLVER.MAX_ITER = 30000\n",
        "\n",
        "# Learning Rate Schedule: Steps at which the learning rate is reduced.\n",
        "# Reduce LR at ~70% and ~90% of MAX_ITER.\n",
        "cfg.SOLVER.STEPS = (int(0.7 * cfg.SOLVER.MAX_ITER), int(0.9 * cfg.SOLVER.MAX_ITER))\n",
        "cfg.SOLVER.GAMMA = 0.1 # Factor to reduce learning rate by at each step\n",
        "\n",
        "# Checkpoint Period: How often to save model weights (e.g., model_0000999.pth).\n",
        "# Setting this ensures intermediate checkpoints are saved during training.\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 200\n",
        "print(f\"Checkpoints will be saved every {cfg.SOLVER.CHECKPOINT_PERIOD} iterations.\")\n",
        "\n",
        "# Evaluation Period: How often to run evaluation on the validation set (cfg.DATASETS.TEST).\n",
        "cfg.TEST.EVAL_PERIOD = 200 # Evaluate every 500 iterations. Adjust as needed.\n",
        "\n",
        "# ROI Heads Batch Size: Number of RoIs (proposed regions) per image used for training the classification/box head.\n",
        "# 128 or 256 are common values. Usually doesn't need much tuning unless memory is extremely tight.\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "\n",
        "# --- Output Directory ---\n",
        "# Where logs, checkpoints, and final model (`model_final.pth`) will be saved.\n",
        "cfg.OUTPUT_DIR = \"./output_detection\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory set to: {cfg.OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#   3. Customize the Trainer (Optional but Recommended for COCO Eval)\n",
        "# ==============================================================================\n",
        "\n",
        "# DefaultTrainer provides a standard training loop.\n",
        "# We subclass it to explicitly use COCOEvaluator for our COCO-formatted validation data.\n",
        "class CustomTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        \"\"\"\n",
        "        Builds the evaluator for the validation dataset.\n",
        "        \"\"\"\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "            os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        # Use COCOEvaluator for datasets registered with register_coco_instances\n",
        "        return COCOEvaluator(dataset_name, cfg, distributed=False, output_dir=output_folder)\n",
        "\n",
        "    # You can add other customizations here, e.g., custom data augmentation hooks\n",
        "    # def build_hooks(self):\n",
        "    #     hooks = super().build_hooks()\n",
        "    #     hooks.insert(-1, YourCustomHook()) # Example\n",
        "    #     return hooks\n",
        "\n",
        "print(\"\\n--- Initializing Trainer ---\")\n",
        "# Initialize the trainer with our configuration\n",
        "trainer = CustomTrainer(cfg)\n",
        "\n",
        "# Load the pre-trained weights specified in cfg.MODEL.WEIGHTS.\n",
        "# resume=False tells it *not* to look for a checkpoint in cfg.OUTPUT_DIR to resume from,\n",
        "# but instead to load the weights defined in cfg.MODEL.WEIGHTS. This is crucial for starting fine-tuning.\n",
        "trainer.resume_or_load(resume=True)\n",
        "print(\"Trainer initialized and pre-trained weights requested for loading.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#   4. Start Training\n",
        "# ==============================================================================\n",
        "print(f\"\\n--- Starting Training for {cfg.SOLVER.MAX_ITER} iterations ---\")\n",
        "print(f\"Checkpoints will be saved in: {cfg.OUTPUT_DIR}\")\n",
        "print(f\"Evaluation will run every {cfg.TEST.EVAL_PERIOD} iterations.\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n--- Training Finished Successfully ---\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Training Interrupted by Error ---\")\n",
        "    print(e)\n",
        "    # Consider adding more detailed error logging here if needed\n",
        "    raise # Re-raise the exception to see the full traceback\n",
        "\n",
        "# ==============================================================================\n",
        "#   5. Optional: Inference with the Trained Model\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Configuration for Inference ---\n",
        "# Load the final trained weights\n",
        "inference_cfg = cfg.clone() # Use a copy of the config\n",
        "inference_cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") # Path to trained weights\n",
        "\n",
        "# Check if the final model exists before proceeding\n",
        "if not os.path.exists(inference_cfg.MODEL.WEIGHTS):\n",
        "    print(f\"\\n--- Inference Skipped: Final model not found at {inference_cfg.MODEL.WEIGHTS} ---\")\n",
        "    print(\"Ensure training completed successfully and the file was saved.\")\n",
        "else:\n",
        "    print(f\"\\n--- Running Inference with Final Model ---\")\n",
        "    print(f\"Loading weights from: {inference_cfg.MODEL.WEIGHTS}\")\n",
        "\n",
        "    # Set the threshold for detections (boxes with scores lower than this will be filtered out)\n",
        "    inference_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
        "    print(f\"Inference score threshold set to: {inference_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}\")\n",
        "\n",
        "    # Create the predictor instance\n",
        "    predictor = DefaultPredictor(inference_cfg)\n",
        "    print(\"Predictor initialized.\")\n",
        "\n",
        "    # --- Load Validation Metadata and Data ---\n",
        "    try:\n",
        "        val_metadata = MetadataCatalog.get(val_dataset_name)\n",
        "        dataset_dicts = DatasetCatalog.get(val_dataset_name) # Load registered val data info\n",
        "\n",
        "        # --- Run Inference on a Random Validation Image ---\n",
        "        if not dataset_dicts:\n",
        "             print(\"Validation dataset dictionary is empty. Cannot perform inference example.\")\n",
        "        else:\n",
        "            sample = random.choice(dataset_dicts) # Get a random sample dictionary\n",
        "            image_path = sample[\"file_name\"]\n",
        "            print(f\"Running inference on random validation image: {image_path}\")\n",
        "\n",
        "            if not os.path.exists(image_path):\n",
        "                 print(f\"Error: Image file not found at {image_path}\")\n",
        "            else:\n",
        "                img = cv2.imread(image_path) # Load the image using OpenCV\n",
        "\n",
        "                if img is None:\n",
        "                    print(f\"Error: Could not read image file {image_path}\")\n",
        "                else:\n",
        "                    # Perform prediction\n",
        "                    outputs = predictor(img) # Input format is BGR (standard for OpenCV)\n",
        "\n",
        "                    # --- Visualize the Predictions ---\n",
        "                    print(\"Visualizing predictions...\")\n",
        "                    v = Visualizer(img[:, :, ::-1], # Convert BGR to RGB for visualizer\n",
        "                                   metadata=val_metadata,\n",
        "                                   scale=0.8)\n",
        "                    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\")) # Draw boxes, labels, scores\n",
        "                    output_image = out.get_image()[:, :, ::-1] # Convert back to BGR for OpenCV display/saving\n",
        "\n",
        "                    # --- Display or Save the Result ---\n",
        "                    output_image_path = os.path.join(cfg.OUTPUT_DIR, \"inference_example.jpg\")\n",
        "                    cv2.imwrite(output_image_path, output_image)\n",
        "                    print(f\"Saved visualization to: {output_image_path}\")\n",
        "\n",
        "                    # # Option 1: Show in a window (if you have a GUI environment)\n",
        "                    # cv2.imshow(\"Detection Result\", output_image)\n",
        "                    # print(\"Press any key in the 'Detection Result' window to close.\")\n",
        "                    # cv2.waitKey(0) # Wait indefinitely for a key press\n",
        "                    # cv2.destroyAllWindows()\n",
        "\n",
        "                    # Option 2: If running in a headless environment (like Colab/Kaggle/server),\n",
        "                    # viewing the saved file (inference_example.jpg) is necessary.\n",
        "                    print(\"\\nInference example complete.\")\n",
        "\n",
        "    except KeyError:\n",
        "        print(f\"Error: Could not retrieve metadata or data for validation set '{val_dataset_name}'. Inference example skipped.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the inference example: {e}\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkTkayO3RILK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
